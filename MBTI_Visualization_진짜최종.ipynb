{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuyeongKime2/Visualization-of-topics-by-MBTI/blob/main/MBTI_Visualization_%EC%A7%84%EC%A7%9C%EC%B5%9C%EC%A2%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read_csv**"
      ],
      "metadata": {
        "id": "RQ-EmChTdr7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2Hcr2vUcKvU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 업로드된 파일의 파일 이름과 내용을 가져오기\n",
        "filename = list(uploaded.keys())[0]\n",
        "file_contents = uploaded[filename]\n",
        "\n",
        "# 파일 내용을 UTF-8로 디코딩\n",
        "decoded_content = file_contents.decode('utf-8')\n",
        "\n",
        "# StringIO를 사용하여 파일과 유사한 객체 생성\n",
        "file_like_object = io.StringIO(decoded_content)\n",
        "\n",
        "# CSV 파일을 Pandas DataFrame으로 읽기\n",
        "df = pd.read_csv(file_like_object)\n",
        "\n",
        "#데이터 확인\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-PiHglBjrgo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CountVectorizer 객체 생성 및 토픽 중요도 계산**\n",
        "각 문서에 대해 토픽 분포를 계산"
      ],
      "metadata": {
        "id": "WDXZdp7_eFdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer 객체 생성\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# 텍스트 벡터화 - 필요한 경우 열 이름 조정\n",
        "# 'Text'가 올바른 열이 아닌 경우 실제 이름으로 바꿉니다.\n",
        "# Check the column names in your DataFrame\n",
        "print(df.columns)\n",
        "\n",
        "# 'actual_column_name'을 텍스트 데이터가 포함된 올바른 열 이름으로 변경\n",
        "# 제공된 전역 변수를 기반으로 'posts'가 적합한 후보로 보임\n",
        "X = vectorizer.fit_transform(df['posts'])\n",
        "\n",
        "# LDA 모델 생성 및 적합\n",
        "num_topics = 10  # 적절한 토픽 수 선택\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "# 각 문서에 대한 토픽 분포 계산\n",
        "topic_distributions = lda.transform(X)\n",
        "\n",
        "# 'type'으로 MBTI 열 이름 설정. DataFrame의 실제 열 이름으로 변경해야 할 수 있습니다.\n",
        "mbti_column_name = 'type'\n",
        "\n",
        "# 결과 출력\n",
        "for i, topic_dist in enumerate(topic_distributions):\n",
        "    print(f\"Document {i} (MBTI: {df[mbti_column_name][i]}): {topic_dist}\")"
      ],
      "metadata": {
        "id": "NreDfcSJf9EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **각 토픽별로 확률이 높은 단어들을 문자열로 출력**"
      ],
      "metadata": {
        "id": "2HSytNwIp2le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# TF-IDF 벡터화 또는 CountVectorizer를 사용한 경우\n",
        "feature_names = vectorizer.get_feature_names_out() # Use get_feature_names_out()\n",
        "\n",
        "# 각 토픽별로 확률이 높은 단어들을 문자열로 출력\n",
        "top_n_words = 10  # 각 토픽에서 상위 n개의 단어를 출력\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_words_indices = topic.argsort()[-top_n_words:][::-1]\n",
        "    top_words = [feature_names[i] for i in top_words_indices]\n",
        "    top_words_str = \", \".join(top_words)\n",
        "    print(f\"토픽 {topic_idx+1}: {top_words_str}\")"
      ],
      "metadata": {
        "id": "Nl-ewXyRW8v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **히트맵 생성 :집계된 토픽 중요도를 히트맵으로 시각화**"
      ],
      "metadata": {
        "id": "e7gCTDL6gMs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install koreanize-matplotlib"
      ],
      "metadata": {
        "id": "7sqwyY1HR3Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import koreanize_matplotlib\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df_heatmap = pd.DataFrame(topic_importance, index=[f'Topic {i}' for i in range(lda.n_components)])\n",
        "\n",
        "# 히트맵 생성\n",
        "plt.figure(figsize=(13, 13)) #크기 설정\n",
        "sns.heatmap(df_heatmap, annot=True, cmap='YlGnBu', cbar=True)\n",
        "plt.title('MBTI 별 토픽 중요도 히트맵')\n",
        "plt.xlabel('MBTI 유형')\n",
        "plt.ylabel('토픽')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2yqL8IgIgPQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **워드클라우드 생성**\n",
        "\n",
        "상위 10위의 MBTI Topic이 담긴 텍스트 파일을 변수에 할당해서 워드클라우드 출력"
      ],
      "metadata": {
        "id": "nymKuQqx2Kdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install fonts-nanum\n",
        "!fc-cache -fv\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 업로드된 파일의 파일 이름과 내용을 가져오기\n",
        "filename = list(uploaded.keys())[0]\n",
        "file_contents = uploaded[filename]\n",
        "\n",
        "# 파일 내용을 문자열로 디코딩\n",
        "text = file_contents.decode('utf-8') # Decode the bytes to string\n",
        "\n",
        "# 중복 단어 제거\n",
        "words = text.split()\n",
        "unique_words = set(words)\n",
        "unique_text = \" \".join(unique_words)\n",
        "\n",
        "# 워드클라우드 생성\n",
        "wc = WordCloud(width=800,            # 워드클라우드 이미지의 너비 설정\n",
        "               height=400,           # 워드클라우드 이미지의 높이 설정\n",
        "               background_color='white', # 워드클라우드 배경색 설정\n",
        "               colormap='tab20b',     # 워드클라우드 색상 맵 설정\n",
        "               font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf'  # 워드클라우드에 사용할 폰트 파일 경로 설정\n",
        "              ).generate(unique_text) # unique_text를 이용하여 워드클라우드 생성\n",
        "\n",
        "# 워드클라우드 출력\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"텍스트 파일 워드클라우드 (중복 제거)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "amYFKWUuwsac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}